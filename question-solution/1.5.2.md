# Solution to Review Question

by Qiang Gao, updated at May 15, 2017

---

## Chapter 1 Finite-Sample Properties of OLS

### Section 5 Relation to Maximum Likelihood

...

#### Review Question 1.5.2 (Maximizing joint log likelihood)

Consider maximizing (the log of) the _joint_ likelihood

$$
f_{ \mathbf{y}, \mathbf{X} } ( \mathbf{y}, \mathbf{X}; \tilde{ \boldsymbol{ \zeta } } ) =
f_{ \mathbf{y} \mid \mathbf{X} } ( \mathbf{y} \mid \mathbf{X}; \tilde{ \boldsymbol{ \theta } } ) \cdot
f_{ \mathbf{X} } ( \mathbf{X} ; \tilde{ \boldsymbol{ \psi } } )
\tag{1.5.2}
$$

for the classical regression model, where $$ \tilde{ \boldsymbol{ \theta } } = ( \tilde{ \boldsymbol{ \beta } }, \tilde{ \sigma }^2 )'$$ and $$ \log f_{ \mathbf{y} \mid \mathbf{X} } ( \mathbf{y} \mid \mathbf{X}; \tilde{ \boldsymbol{ \theta } } ) $$ is given by

$$
\log L ( \tilde{ \boldsymbol{ \beta } }, \tilde{ \sigma }^2 ) =
- \frac{n}{2} \log ( 2 \pi ) -
\frac{n}{2} \log ( \tilde{ \sigma }^2 ) -
\frac{1}{ 2 \tilde{ \sigma }^2 }
( \mathbf{y} - \mathbf{X} \tilde{ \boldsymbol{ \beta } } )'
( \mathbf{y} - \mathbf{X} \tilde{ \boldsymbol{ \beta } } ).
\tag{1.5.5}
$$

You would parameterize the marginal likelihood $$ f( \mathbf{X} ; \tilde{ \boldsymbol{ \psi } } ) $$ and take the log of (1.5.2) to obtain the objective function to be maximized over $$ \boldsymbol{ \zeta } \equiv ( \boldsymbol{ \theta }', \boldsymbol{ \psi }' )' $$. (a) What is the ML estimator of $$ \boldsymbol{ \theta } \equiv ( \boldsymbol{ \beta }', \sigma^2 )' $$? (b) Derive the Cramer-Rao bound for $$ \boldsymbol{ \beta } $$.

##### Solution

(a) Taking log of (1.5.2),

$$
\begin{align}
\log f_{ \mathbf{y}, \mathbf{X} } ( \mathbf{y}, \mathbf{X}; \tilde{ \boldsymbol{ \zeta } } ) =
\log f_{ \mathbf{y} \mid \mathbf{X} } ( \mathbf{y} \mid \mathbf{X}; \tilde{ \boldsymbol{ \theta } } ) +
\log f_{ \mathbf{X} } ( \mathbf{X} ; \tilde{ \boldsymbol{ \psi } } ).
\tag{1}
\end{align}
$$

The ML estimator of $$ \boldsymbol{ \theta } $$ maximizing (1) will be exactly the same as that of maximizing $$ \log f_{ \mathbf{y} \mid \mathbf{X} } ( \mathbf{y} \mid \mathbf{X}; \tilde{ \boldsymbol{ \theta } } ) $$, because $$ \tilde{ \boldsymbol{ \theta } } $$ does not appear in $$ \log f_{ \mathbf{X} } ( \mathbf{X} ; \tilde{ \boldsymbol{ \psi } } ) $$, the first-order conditions of the two maximization will be the same.

(b) By the information matrix equality,

$$
\begin{align}
\mathbf{I} ( \boldsymbol{ \zeta } ) & =
- \mathrm{E} \left[
\frac{ \partial^2 \log L ( \boldsymbol{ \zeta } ) }
{ \partial \tilde{ \boldsymbol{ \zeta } } \,
\partial \tilde{ \boldsymbol{ \zeta } }' }
\right]
\\ & =
- \mathrm{E} \left[
\begin{matrix}
\frac{ \partial^2 \log L ( \boldsymbol{ \theta }, \boldsymbol{ \psi } ) }
{ \partial \tilde{ \boldsymbol{ \theta } } \,
\partial \tilde{ \boldsymbol{ \theta } }' }
&
\frac{ \partial^2 \log L ( \boldsymbol{ \theta }, \boldsymbol{ \psi } ) }
{ \partial \tilde{ \boldsymbol{ \theta } } \,
\partial \tilde{ \boldsymbol{ \psi } }' }
\\
\frac{ \partial^2 \log L ( \boldsymbol{ \theta }, \boldsymbol{ \psi } ) }
{ \partial \tilde{ \boldsymbol{ \psi } } \,
\partial \tilde{ \boldsymbol{ \theta } }' }
&
\frac{ \partial^2 \log L ( \boldsymbol{ \theta }, \boldsymbol{ \psi } ) }
{ \partial \tilde{ \boldsymbol{ \psi } } \,
\partial \tilde{ \boldsymbol{ \psi } }' }
\end{matrix}
\right]
\\ & =
- \mathrm{E} \left[
\begin{matrix}
\frac{ \partial^2 \log L ( \boldsymbol{ \theta }, \boldsymbol{ \psi } ) }
{ \partial \tilde{ \boldsymbol{ \theta } } \,
\partial \tilde{ \boldsymbol{ \theta } }' }
&
\mathbf{0}
\\
\mathbf{0}
&
\frac{ \partial^2 \log L ( \boldsymbol{ \theta }, \boldsymbol{ \psi } ) }
{ \partial \tilde{ \boldsymbol{ \psi } } \,
\partial \tilde{ \boldsymbol{ \psi } }' }
\end{matrix}
\right],
\end{align}
$$

since $$ \partial^2 \log L ( \boldsymbol{ \theta }, \boldsymbol{ \psi } ) / ( \partial \tilde{ \boldsymbol{ \theta } } \,
\partial \tilde{ \boldsymbol{ \psi } }' ) = \mathbf{0} $$. Thus the information matrix $$ \mathbf{I} ( \boldsymbol{ \zeta } ) $$ is block diagonal with its first block corresponding to $$ \boldsymbol{ \theta } $$ and the second block corresponding to $$ \boldsymbol{ \psi } $$. Its inverse is also block diagonal, with its first block being the inverse of

$$
- \mathrm{E} \left[
\frac{ \partial^2 \log L ( \boldsymbol{ \theta }, \boldsymbol{ \psi } ) }
{ \partial \tilde{ \boldsymbol{ \theta } } \,
\partial \tilde{ \boldsymbol{ \theta } }' }
\right]
=
- \mathrm{E} \left[
\frac{ \partial^2 \log L ( \boldsymbol{ \theta } ) }
{ \partial \tilde{ \boldsymbol{ \theta } } \,
\partial \tilde{ \boldsymbol{ \theta } }' }
\right].
$$

So the Cramer-Rao bound for $$ \boldsymbol{ \theta } $$ is the negative of the inverse of the expected value of (1.5.12) in the text. The expectation, however, is over $$ \mathbf{y} $$ _and_ $$ \mathbf{X} $$ because here the density is a _joint_ density. Therefore, the Cramer-Rao bound for $$ \boldsymbol{ \beta } $$ is $$ \sigma^2 [ \mathrm{E} ( \mathbf{X}' \mathbf{X} ) ]^{-1} $$.

---

Copyright Â©2017 by Qiang Gao