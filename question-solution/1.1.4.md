# Solution to Review Question

by Qiang Gao, updated at Mar 13, 2017

---

## Chapter 1 Finite-Sample Properties of OLS

### Section 1 The Classical Linear Regression Model

...

#### Review Question 1.1.4 (Normally distributed ramdom sample)
Consider a random sample on consumption and disposable income, $$ ( CON_i, YD_i ) $$, $$ ( i = 1, 2, \ldots, n ) $$. Suppose the joint distribution of $$ ( CON_i, YD_i ) $$ (which is the same across $$ i $$ because of the random sample assumption) is normal. Clearly, Assumption 1.3 is satisfied; the rank of $$ \mathbf{X} $$ would be less than $$ K $$ only by pure accident. Show that the other assumptions, Assumptions 1.1, 1.2, and 1.4, are satisfied. **Hint:** if two random variables, $$ y $$ and $$ x $$, are jointly normally distributed, then the conditional expectation is linear in $$ x $$, i.e.,

$$
\mathrm{E} ( y \mid x ) = \beta_1 + \beta_2 x,
$$

and the conditional variance, $$ \mathrm{Var} ( y \mid x ) $$, does not depend on $$ x $$. Here, the fact that the distribution is the same across $$ i $$ is important; if the distribution differed across $$ i $$, $$ \beta_1 $$ and $$ \beta_2 $$ could vary across $$ i $$.

##### Solution (with flaw)

(1) We _define_ the error term as

$$
\varepsilon_i = CON_i - \beta_1 - \beta_2 YD_i,
$$

then Assumption 1.1 is satisfied.

(2) Because

$$
\begin{align}
\mathrm{E} (\varepsilon_i \mid \mathbf{X})
& =
\mathrm{E} ( CON_i - \beta_1 - \beta_2 YD_i \mid \mathbf{X} )
&&
\text{(definition of $\varepsilon_i$)}
\\ & =
\mathrm{E} ( CON_i \mid YD_i ) - \beta_1 - \beta_2 YD_i
&&
\text{(linearity of conditional expectations)}
\\ & =
\beta_1 + \beta_2 \mathit{YD}_i - \beta_1 - \beta_2 \mathit{YD}_i
&&
\text{(hint)}
\\ & = 0,
\end{align}
$$

Assumptoin 1.2 holds.

(3) To prove Assumption 1.4,

$$
\begin{align}
\mathrm{E} ( \varepsilon_i^2 \mid \mathbf{X} )
& =
\mathrm{Var} ( \varepsilon_i \mid \mathbf{X} ) + \mathrm{E} ( \varepsilon_i \mid \mathbf{X} )^2
&&
\text{(definition of $\mathrm{Var} (\cdot)$)}
\\ & =
\mathrm{Var} ( \varepsilon_i \mid \mathbf{X} )
&&
\text{(Assumption 1.2)}
\\ & =
\mathrm{Var} ( CON_i - \beta_1 - \beta_2 YD_i \mid YD_i )
&&
\text{(definition of $\varepsilon_i$)}
\\ & =
\mathrm{Var} ( CON_i \mid YD_i )
&&
\text{($\mathrm{Var} (ax + b) = a^2 \mathrm{Var} (x)$ )}
\\ & = \sigma^2 > 0,
&&
\text{(hint)}
\end{align}
$$

and

$$
\begin{align}
\mathrm{E} ( \varepsilon_i \varepsilon_j \mid \mathbf{X} )
& =
\mathrm{E} ( \varepsilon_i \mid \mathbf{x}_i ) \mathrm{E} ( \varepsilon_j \mid \mathbf{x}_j )
&&
\text{(Review Question 1.1.2)}
\\ & = 0.
&&
\text{(Assumption 1.2)}
\end{align}
$$

---

Copyright Â©2017 by Qiang Gao