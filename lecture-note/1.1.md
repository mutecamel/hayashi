# Lecture Notes on Econometrics

by Qiang Gao, updated at March 26, 2018

---

## Chapter 1 Finite-Sample Properties of OLS

### Section 1 The Classical Linear Regression Model

...

#### The Big Picture of Econometrics

Its a bridge between **model** and **data**. Model is some theoretic mathematical formulation. Data is collected/measured from the real world according to the definition of variables.

The bridge is in two ways:

1. (from data to model) Parameter Estimation.
2. (from model to data) Hypothesis Test.

#### Assumption 1.1 (linearity)

##### 1. It's a tautology

Because the error term $$ \varepsilon_i $$ is defined as

$$
\varepsilon_i = y_i - \mathbf{x}_i \cdot \boldsymbol\beta,
$$

the equality in (1.1.1) trivially holds true by definition.

Equation (1.1.1) only restricts a linear functional relationship between $$y$$ and $$ \mathbf{x} $$, nothing more.

##### 2. Nonlinearity can be linearized

The linearity assumption is not so much restrictive, because any nonlinear function can be easily [linearized](../supplements/taylor-linearization.md).

##### 3. The knowns are $$( y_i, \mathbf{x}_i $$) and the unknowns are $$( \boldsymbol\beta, \varepsilon_i )$$

##### 4. $$ \boldsymbol\beta $$ is of primary interest

$$ \beta $$ means marginal separate effects.

##### 5. $$ \varepsilon_i $$ is of primary concern

$$\varepsilon_i$$ should not depend on $$ \mathbf{x} $$

##### 6. marginal separate effect relies on total differentiation

- explicit equation
- implicit equation
- differential vs. elasticity

##### 7. variables are usually transformed (in log)

By the rules of differentiation

$$
\frac{d \ln x}{dx} = \frac{1}{x},
$$

we can write it in total differential form as

$$
d \ln x = \frac{dx}{x}.
$$

Similarly,

$$
d \ln y = \frac{dy}{y}.
$$

So

$$
\frac{d \ln y}{d \ln x} = \frac{d y / y}{d x / x}
$$

coincides with the definition of elasticity. It is of this reason that
in economics, variables are often expressed in logs rather than in
levels in equations.

#### Assumption 1.2 (strict exogeneity)

##### Joint Distribution

$$
f_{Y,X}(y, x) \qquad \oint f_{Y,X}(y, x)\,dx\,dy = 1
$$

##### Marginal Distribution

$$
\begin{align}
f_{Y} (y) \equiv \oint f_{Y,X}(y, x) \, dx && \oint f_{Y} (y) \, dy = 1 \\
f_{X} (x) \equiv \oint f_{Y,X}(y, x) \, dy && \oint f_{X} (x) \, dx = 1 
\end{align}
$$

##### Conditional Distribution

##### (Unconditional) Expectation

The (unconditional) expectation $$\mathrm{E}(x)$$ is defined as

$$
\mathrm{E}(x) = \int x f(y, x) \, dy dx
$$

##### Conditional Expectation

If $$(y, x)$$ are jointly distributed random variables, where their joint p.d.f. is expressed as $$f(y, x)$$, then $$\mathrm{E} (y | x)$$ is defined
as

$$
\mathrm{E} (y|x) = \int_{-\infty}^{+\infty} y \frac{ f(y, x) }{ \int_{-\infty}^{+\infty} f(y, x) dy } dy,
$$

where $$\int_{-\infty}^{+\infty} f(y, x) dy$$ is the definition of the marginal distribution of $$x$$. In words, the expectation of $$y$$ conditional on $$x$$ is the weighted average of $$y$$, where the weighting is the conditional probability density.

##### Law of Total Expectations

$$
\mathrm{E} ( \mathrm{E} (y | x) ) = \mathrm{E} (y).
$$

##### Law of Iterated Expectations

$$
\mathrm{E} ( \mathrm{E} (y | x, z) | z ) = \mathrm{E} (y | z).
$$

##### Moment

The $$k$$-th order moment of a random variable $$x$$ is defined as

$$
\mathrm{E}(x^k)
$$

##### Variance

$$
\begin{align}
\mathrm{Var}(x) &= \mathrm{E} [ (x - \mathrm{E} (x))^2 ] && \text{(definition)} \\
&= \mathrm{E}(x^2)- E(x)^2 && \text{(formula)}
\end{align}
$$

##### Covariance

$$
\begin{align}
\mathrm{Cov} (x, y) &= \mathrm{E} [ (x - \mathrm{E} (x) )( y - \mathrm{E} (y)) ] && \text{(definition)} \\
&= \mathrm{E} (xy) - \mathrm{E}(x) \mathrm{E} (y) && \text{(formula)}
\end{align}
$$

##### Correlation Coefficient

$$
\rho_{x,y} = \frac{\mathrm{Cov} (x,y)}{\sqrt {\mathrm{Var} (x) \mathrm{Var} (y)}} \in [-1, 1]
$$

##### Linearity of Expectation

$$
\mathrm{E} (ax + b) = a \mathrm{E} (x) + b
$$

##### Nonlinearity of Variance

$$
\mathrm{Var} (ax + b) = a^2 \mathrm{Var} (x).
$$

#### Assumption 1.3 (no multicollinearity)

- perfect multicollinearity _can_ occur in rare conditions as long as its _measure_ is zero.

#### Assumption 1.4 (spherical error variance)

$$
\mathbf{x} \mathbf{x}'
\equiv
\begin{bmatrix}
x_1^2 & \cdots & x_1 x_n \\
\vdots & \ddots & \vdots \\
x_n x_1 & \cdots & x_n^2
\end{bmatrix}
$$

$$
\mathrm{E}
\begin{bmatrix}
a_{11} & \cdots & a_{n1} \\
\vdots & \ddots & \vdots \\
a_{m1} & \cdots & a_{mn}
\end{bmatrix}
\equiv
\begin{bmatrix}
\mathrm{E} (a_{11}) & \cdots & \mathrm{E} (a_{n1}) \\
\vdots & \ddots & \vdots \\
\mathrm{E} (a_{m1}) & \cdots & \mathrm{E} (a_{mn})
\end{bmatrix},
$$

$$
\mathrm{Var} ( \mathbf{x} )
\equiv
\mathrm{E} [ ( \mathbf{x} - \overline{\mathbf{x}} ) ( \mathbf{x} - \overline{\mathbf{x}} )' ]
$$

---

Copyright Â©2018 by Qiang Gao